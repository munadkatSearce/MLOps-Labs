{
  "pipelineSpec": {
    "components": {
      "comp-deploy-to-run": {
        "executorLabel": "exec-deploy-to-run",
        "inputDefinitions": {
          "parameters": {
            "tar_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-deploy-to-run-2": {
        "executorLabel": "exec-deploy-to-run-2",
        "inputDefinitions": {
          "parameters": {
            "tar_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-deploy-to-run-3": {
        "executorLabel": "exec-deploy-to-run-3",
        "inputDefinitions": {
          "parameters": {
            "tar_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-get-air-data": {
        "executorLabel": "exec-get-air-data",
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-sequential-model": {
        "executorLabel": "exec-sequential-model",
        "inputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "model_type": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "tar_path": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-sequential-model-2": {
        "executorLabel": "exec-sequential-model-2",
        "inputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "model_type": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "tar_path": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-sequential-model-3": {
        "executorLabel": "exec-sequential-model-3",
        "inputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "model_type": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "tar_path": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-deploy-to-run": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "deploy_to_run"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef deploy_to_run(\n    tar_path: str,\n    output_data_path: OutputPath(\"Dataset\")\n    ):\n    from google.cloud import storage\n    import os\n    from github import Github\n\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(tar_path.split(\"/\")[0])\n    blob = bucket.blob(\"/\".join(tar_path.split(\"/\")[1:]))\n    blob.download_to_filename(\"/tmp/model.pth.tar\")\n\n    g = Github(\"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\")\n\n    repo = g.get_repo(\"munadkatSearce/darts-model-serving\")\n    contents = repo.get_contents(\"model.pth.tar\", ref=\"main\")\n\n    model_file = open(\"/tmp/model.pth.tar\", \"rb\")\n    file_content=model_file.read()\n    model_file.close()\n\n    repo.update_file(path=contents.path, message=\"new_model_trained\", content=file_content, sha=contents.sha)\n\n"
            ],
            "image": "gcr.io/impact-analytics-sandbox/base_container:v3"
          }
        },
        "exec-deploy-to-run-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "deploy_to_run"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef deploy_to_run(\n    tar_path: str,\n    output_data_path: OutputPath(\"Dataset\")\n    ):\n    from google.cloud import storage\n    import os\n    from github import Github\n\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(tar_path.split(\"/\")[0])\n    blob = bucket.blob(\"/\".join(tar_path.split(\"/\")[1:]))\n    blob.download_to_filename(\"/tmp/model.pth.tar\")\n\n    g = Github(\"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\")\n\n    repo = g.get_repo(\"munadkatSearce/darts-model-serving\")\n    contents = repo.get_contents(\"model.pth.tar\", ref=\"main\")\n\n    model_file = open(\"/tmp/model.pth.tar\", \"rb\")\n    file_content=model_file.read()\n    model_file.close()\n\n    repo.update_file(path=contents.path, message=\"new_model_trained\", content=file_content, sha=contents.sha)\n\n"
            ],
            "image": "gcr.io/impact-analytics-sandbox/base_container:v3"
          }
        },
        "exec-deploy-to-run-3": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "deploy_to_run"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef deploy_to_run(\n    tar_path: str,\n    output_data_path: OutputPath(\"Dataset\")\n    ):\n    from google.cloud import storage\n    import os\n    from github import Github\n\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.get_bucket(tar_path.split(\"/\")[0])\n    blob = bucket.blob(\"/\".join(tar_path.split(\"/\")[1:]))\n    blob.download_to_filename(\"/tmp/model.pth.tar\")\n\n    g = Github(\"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\")\n\n    repo = g.get_repo(\"munadkatSearce/darts-model-serving\")\n    contents = repo.get_contents(\"model.pth.tar\", ref=\"main\")\n\n    model_file = open(\"/tmp/model.pth.tar\", \"rb\")\n    file_content=model_file.read()\n    model_file.close()\n\n    repo.update_file(path=contents.path, message=\"new_model_trained\", content=file_content, sha=contents.sha)\n\n"
            ],
            "image": "gcr.io/impact-analytics-sandbox/base_container:v3"
          }
        },
        "exec-get-air-data": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_air_data"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_air_data(\n    #bq_table: str,\n    output_data_path: OutputPath(\"Dataset\")\n):\n    from google.cloud import bigquery\n    import pandas as pd\n    bqclient = bigquery.Client(project=\"impact-analytics-sandbox\")\n\n    # Download query results.\n    query_string = \"\"\"\n    SELECT *\n    FROM `impact-analytics-sandbox.poc_dataset.AirPassengersDataset`\n    \"\"\"\n    # get dataframe by querying bigquery table\n    air_df = (\n        bqclient.query(query_string)\n            .result()\n            .to_dataframe(\n            create_bqstorage_client=True,\n        )\n    )\n\n    air_df.to_csv(output_data_path,index=False)\n    print(output_data_path)\n\n"
            ],
            "image": "gcr.io/impact-analytics-sandbox/create_base_image:v1"
          }
        },
        "exec-sequential-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "sequential_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef sequential_model(\n    dataset:  Input[Dataset],\n    model_type: str,\n    model: Output[Model],\n    metrics: Output[Metrics],\n    model_path: OutputPath(\"Model\"),\n) -> NamedTuple('ExampleOutputs', [('tar_path', str)]):\n    import pandas as pd\n    import numpy as np\n    import torch\n    import matplotlib.pyplot as plt\n    import json\n    from darts import TimeSeries\n    from darts.utils.timeseries_generation import (\n        gaussian_timeseries,\n        linear_timeseries,\n        sine_timeseries,\n    )\n    from darts.models import RNNModel\n    from darts.metrics import mape, mse, rmse\n    from darts.dataprocessing.transformers import Scaler\n    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n    from google.cloud import storage\n    import glob\n    import shutil\n    from collections import namedtuple\n    from typing import NamedTuple\n    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n        \"\"\"Uploads a file to the bucket.\"\"\"\n\n        storage_client = storage.Client()\n        bucket = storage_client.bucket(bucket_name)\n        blob = bucket.blob(destination_blob_name)\n\n        blob.upload_from_filename(source_file_name)\n\n        print(\n            \"File {} uploaded to {}.\".format(\n                source_file_name, destination_blob_name\n            )\n        )    \n    # Data Preparation\n    air_df = pd.read_csv(dataset.path)\n    air_df['Month']=pd.to_datetime(air_df['Month'])\n    air_df.sort_values(by=\"Month\",inplace=True)\n    air_df.index = air_df['Month']\n    air_df.drop(\"Month\",inplace=True,axis=1)\n    series = TimeSeries.from_dataframe(air_df)\n    # Create training and validation sets:\n    train, val = series.split_after(pd.Timestamp(\"19590101\"))\n\n    # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n    transformer = Scaler()\n    train_transformed = transformer.fit_transform(train)\n    val_transformed = transformer.transform(val)\n    series_transformed = transformer.transform(series)\n\n    # create month and year covariate series\n    year_series = datetime_attribute_timeseries(\n        pd.date_range(start=series.start_time(), freq=series.freq_str, periods=1000),\n        attribute=\"year\",\n        one_hot=False,\n    )\n    year_series = Scaler().fit_transform(year_series)\n    month_series = datetime_attribute_timeseries(\n        year_series, attribute=\"month\", one_hot=True\n    )\n    covariates = year_series.stack(month_series)\n    cov_train, cov_val = covariates.split_after(pd.Timestamp(\"19590101\"))\n\n\n    #setting hyperparameters\n    hidden_dim=20\n    dropout=0\n    batch_size=16\n    epochs=300\n    learning_rate=1e-3\n    optimizer_kwargs={\"lr\":learning_rate }\n    model_name=\"Air_\"+model_type\n    log_tensorboard=True\n    random_state=42\n    training_length=20\n    input_chunk_length=14\n    force_reset=True\n    save_checkpoints=True\n    # Model Creation\n    my_model = RNNModel(\n        model=model_type,\n        hidden_dim=hidden_dim,\n        dropout=dropout,\n        batch_size=batch_size,\n        n_epochs=epochs,\n        optimizer_kwargs=optimizer_kwargs,\n        model_name=model_name,\n        log_tensorboard=log_tensorboard,\n        random_state=random_state,\n        training_length=training_length,\n        input_chunk_length=input_chunk_length,\n        force_reset=True,\n        save_checkpoints=True,\n    )\n    my_model.fit(\n        train_transformed,\n        future_covariates=covariates,\n        val_series=val_transformed,\n        val_future_covariates=covariates,\n        verbose=False,\n    )\n    # metadata about model\n    model.metadata[\"hidden_dim\"] = hidden_dim\n    model.metadata[\"dropout\"] = dropout\n    model.metadata[\"batch_size\"] = batch_size\n    model.metadata[\"n_epochs\"]=  epochs\n    model.metadata[\"learning rate\"] = learning_rate\n    #model.metadata[\"model_name\"]=\"Air_\"+model_type\n\n    model.metadata[\"random_state\"] = random_state\n    model.metadata[\"training_length\"] = training_length\n    model.metadata[\"input_chunk_length\"] = input_chunk_length\n\n    from datetime import datetime\n    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    k = glob.glob('darts_logs' + \"/**\", recursive=True)\n    blobs_list = []\n    for i in k:\n        if '.' in i:\n            blobs_list.append(i)\n\n    bucket_name = 'impact-analytics-experiments-bucket01'\n\n    # Saving ML models to bucket\n    for blob in blobs_list:\n        print(blob)    \n        source_file_name = blob \n        destination_blob_name = 'model_logs{}/'.format(TIMESTAMP)+blob\n        upload_blob(bucket_name, source_file_name, destination_blob_name)\n        if blob.endswith(\".pth.tar\"):\n            path = bucket_name + \"/\" + destination_blob_name\n\n    my_model.save_model(model_path+ \".pth.tar\")\n\n\n\n    # Evaluating model\n    def eval_model(model):\n        pred_series = model.predict(n=26, future_covariates=covariates)\n        mape1 = mape(pred_series, val_transformed)\n        mse1 = mse(pred_series, val_transformed)\n        rmse1 = rmse(pred_series, val_transformed)\n        print(\"MAPE: {:.2f}%\".format(mape1))\n        print(\"MSE: {:.2f}%\".format(mse1))\n        print(\"RMSE: {:.2f}%\".format(rmse1))\n        metrics.log_metric(\"MAPE\",\"{:.2f}%\".format(mape1))\n        metrics.log_metric(\"MSE\", \"{}\".format(mse1))\n        metrics.log_metric(\"RMSE\", \"{}\".format(rmse1))\n\n    eval_model(my_model)\n    example_output = namedtuple('ExampleOutputs', ['tar_path'])\n    return example_output(path)\n\n"
            ],
            "image": "gcr.io/impact-analytics-sandbox/create_base_image:v1"
          }
        },
        "exec-sequential-model-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "sequential_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef sequential_model(\n    dataset:  Input[Dataset],\n    model_type: str,\n    model: Output[Model],\n    metrics: Output[Metrics],\n    model_path: OutputPath(\"Model\"),\n) -> NamedTuple('ExampleOutputs', [('tar_path', str)]):\n    import pandas as pd\n    import numpy as np\n    import torch\n    import matplotlib.pyplot as plt\n    import json\n    from darts import TimeSeries\n    from darts.utils.timeseries_generation import (\n        gaussian_timeseries,\n        linear_timeseries,\n        sine_timeseries,\n    )\n    from darts.models import RNNModel\n    from darts.metrics import mape, mse, rmse\n    from darts.dataprocessing.transformers import Scaler\n    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n    from google.cloud import storage\n    import glob\n    import shutil\n    from collections import namedtuple\n    from typing import NamedTuple\n    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n        \"\"\"Uploads a file to the bucket.\"\"\"\n\n        storage_client = storage.Client()\n        bucket = storage_client.bucket(bucket_name)\n        blob = bucket.blob(destination_blob_name)\n\n        blob.upload_from_filename(source_file_name)\n\n        print(\n            \"File {} uploaded to {}.\".format(\n                source_file_name, destination_blob_name\n            )\n        )    \n    # Data Preparation\n    air_df = pd.read_csv(dataset.path)\n    air_df['Month']=pd.to_datetime(air_df['Month'])\n    air_df.sort_values(by=\"Month\",inplace=True)\n    air_df.index = air_df['Month']\n    air_df.drop(\"Month\",inplace=True,axis=1)\n    series = TimeSeries.from_dataframe(air_df)\n    # Create training and validation sets:\n    train, val = series.split_after(pd.Timestamp(\"19590101\"))\n\n    # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n    transformer = Scaler()\n    train_transformed = transformer.fit_transform(train)\n    val_transformed = transformer.transform(val)\n    series_transformed = transformer.transform(series)\n\n    # create month and year covariate series\n    year_series = datetime_attribute_timeseries(\n        pd.date_range(start=series.start_time(), freq=series.freq_str, periods=1000),\n        attribute=\"year\",\n        one_hot=False,\n    )\n    year_series = Scaler().fit_transform(year_series)\n    month_series = datetime_attribute_timeseries(\n        year_series, attribute=\"month\", one_hot=True\n    )\n    covariates = year_series.stack(month_series)\n    cov_train, cov_val = covariates.split_after(pd.Timestamp(\"19590101\"))\n\n\n    #setting hyperparameters\n    hidden_dim=20\n    dropout=0\n    batch_size=16\n    epochs=300\n    learning_rate=1e-3\n    optimizer_kwargs={\"lr\":learning_rate }\n    model_name=\"Air_\"+model_type\n    log_tensorboard=True\n    random_state=42\n    training_length=20\n    input_chunk_length=14\n    force_reset=True\n    save_checkpoints=True\n    # Model Creation\n    my_model = RNNModel(\n        model=model_type,\n        hidden_dim=hidden_dim,\n        dropout=dropout,\n        batch_size=batch_size,\n        n_epochs=epochs,\n        optimizer_kwargs=optimizer_kwargs,\n        model_name=model_name,\n        log_tensorboard=log_tensorboard,\n        random_state=random_state,\n        training_length=training_length,\n        input_chunk_length=input_chunk_length,\n        force_reset=True,\n        save_checkpoints=True,\n    )\n    my_model.fit(\n        train_transformed,\n        future_covariates=covariates,\n        val_series=val_transformed,\n        val_future_covariates=covariates,\n        verbose=False,\n    )\n    # metadata about model\n    model.metadata[\"hidden_dim\"] = hidden_dim\n    model.metadata[\"dropout\"] = dropout\n    model.metadata[\"batch_size\"] = batch_size\n    model.metadata[\"n_epochs\"]=  epochs\n    model.metadata[\"learning rate\"] = learning_rate\n    #model.metadata[\"model_name\"]=\"Air_\"+model_type\n\n    model.metadata[\"random_state\"] = random_state\n    model.metadata[\"training_length\"] = training_length\n    model.metadata[\"input_chunk_length\"] = input_chunk_length\n\n    from datetime import datetime\n    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    k = glob.glob('darts_logs' + \"/**\", recursive=True)\n    blobs_list = []\n    for i in k:\n        if '.' in i:\n            blobs_list.append(i)\n\n    bucket_name = 'impact-analytics-experiments-bucket01'\n\n    # Saving ML models to bucket\n    for blob in blobs_list:\n        print(blob)    \n        source_file_name = blob \n        destination_blob_name = 'model_logs{}/'.format(TIMESTAMP)+blob\n        upload_blob(bucket_name, source_file_name, destination_blob_name)\n        if blob.endswith(\".pth.tar\"):\n            path = bucket_name + \"/\" + destination_blob_name\n\n    my_model.save_model(model_path+ \".pth.tar\")\n\n\n\n    # Evaluating model\n    def eval_model(model):\n        pred_series = model.predict(n=26, future_covariates=covariates)\n        mape1 = mape(pred_series, val_transformed)\n        mse1 = mse(pred_series, val_transformed)\n        rmse1 = rmse(pred_series, val_transformed)\n        print(\"MAPE: {:.2f}%\".format(mape1))\n        print(\"MSE: {:.2f}%\".format(mse1))\n        print(\"RMSE: {:.2f}%\".format(rmse1))\n        metrics.log_metric(\"MAPE\",\"{:.2f}%\".format(mape1))\n        metrics.log_metric(\"MSE\", \"{}\".format(mse1))\n        metrics.log_metric(\"RMSE\", \"{}\".format(rmse1))\n\n    eval_model(my_model)\n    example_output = namedtuple('ExampleOutputs', ['tar_path'])\n    return example_output(path)\n\n"
            ],
            "image": "gcr.io/impact-analytics-sandbox/create_base_image:v1"
          }
        },
        "exec-sequential-model-3": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "sequential_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef sequential_model(\n    dataset:  Input[Dataset],\n    model_type: str,\n    model: Output[Model],\n    metrics: Output[Metrics],\n    model_path: OutputPath(\"Model\"),\n) -> NamedTuple('ExampleOutputs', [('tar_path', str)]):\n    import pandas as pd\n    import numpy as np\n    import torch\n    import matplotlib.pyplot as plt\n    import json\n    from darts import TimeSeries\n    from darts.utils.timeseries_generation import (\n        gaussian_timeseries,\n        linear_timeseries,\n        sine_timeseries,\n    )\n    from darts.models import RNNModel\n    from darts.metrics import mape, mse, rmse\n    from darts.dataprocessing.transformers import Scaler\n    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n    from google.cloud import storage\n    import glob\n    import shutil\n    from collections import namedtuple\n    from typing import NamedTuple\n    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n        \"\"\"Uploads a file to the bucket.\"\"\"\n\n        storage_client = storage.Client()\n        bucket = storage_client.bucket(bucket_name)\n        blob = bucket.blob(destination_blob_name)\n\n        blob.upload_from_filename(source_file_name)\n\n        print(\n            \"File {} uploaded to {}.\".format(\n                source_file_name, destination_blob_name\n            )\n        )    \n    # Data Preparation\n    air_df = pd.read_csv(dataset.path)\n    air_df['Month']=pd.to_datetime(air_df['Month'])\n    air_df.sort_values(by=\"Month\",inplace=True)\n    air_df.index = air_df['Month']\n    air_df.drop(\"Month\",inplace=True,axis=1)\n    series = TimeSeries.from_dataframe(air_df)\n    # Create training and validation sets:\n    train, val = series.split_after(pd.Timestamp(\"19590101\"))\n\n    # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n    transformer = Scaler()\n    train_transformed = transformer.fit_transform(train)\n    val_transformed = transformer.transform(val)\n    series_transformed = transformer.transform(series)\n\n    # create month and year covariate series\n    year_series = datetime_attribute_timeseries(\n        pd.date_range(start=series.start_time(), freq=series.freq_str, periods=1000),\n        attribute=\"year\",\n        one_hot=False,\n    )\n    year_series = Scaler().fit_transform(year_series)\n    month_series = datetime_attribute_timeseries(\n        year_series, attribute=\"month\", one_hot=True\n    )\n    covariates = year_series.stack(month_series)\n    cov_train, cov_val = covariates.split_after(pd.Timestamp(\"19590101\"))\n\n\n    #setting hyperparameters\n    hidden_dim=20\n    dropout=0\n    batch_size=16\n    epochs=300\n    learning_rate=1e-3\n    optimizer_kwargs={\"lr\":learning_rate }\n    model_name=\"Air_\"+model_type\n    log_tensorboard=True\n    random_state=42\n    training_length=20\n    input_chunk_length=14\n    force_reset=True\n    save_checkpoints=True\n    # Model Creation\n    my_model = RNNModel(\n        model=model_type,\n        hidden_dim=hidden_dim,\n        dropout=dropout,\n        batch_size=batch_size,\n        n_epochs=epochs,\n        optimizer_kwargs=optimizer_kwargs,\n        model_name=model_name,\n        log_tensorboard=log_tensorboard,\n        random_state=random_state,\n        training_length=training_length,\n        input_chunk_length=input_chunk_length,\n        force_reset=True,\n        save_checkpoints=True,\n    )\n    my_model.fit(\n        train_transformed,\n        future_covariates=covariates,\n        val_series=val_transformed,\n        val_future_covariates=covariates,\n        verbose=False,\n    )\n    # metadata about model\n    model.metadata[\"hidden_dim\"] = hidden_dim\n    model.metadata[\"dropout\"] = dropout\n    model.metadata[\"batch_size\"] = batch_size\n    model.metadata[\"n_epochs\"]=  epochs\n    model.metadata[\"learning rate\"] = learning_rate\n    #model.metadata[\"model_name\"]=\"Air_\"+model_type\n\n    model.metadata[\"random_state\"] = random_state\n    model.metadata[\"training_length\"] = training_length\n    model.metadata[\"input_chunk_length\"] = input_chunk_length\n\n    from datetime import datetime\n    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    k = glob.glob('darts_logs' + \"/**\", recursive=True)\n    blobs_list = []\n    for i in k:\n        if '.' in i:\n            blobs_list.append(i)\n\n    bucket_name = 'impact-analytics-experiments-bucket01'\n\n    # Saving ML models to bucket\n    for blob in blobs_list:\n        print(blob)    \n        source_file_name = blob \n        destination_blob_name = 'model_logs{}/'.format(TIMESTAMP)+blob\n        upload_blob(bucket_name, source_file_name, destination_blob_name)\n        if blob.endswith(\".pth.tar\"):\n            path = bucket_name + \"/\" + destination_blob_name\n\n    my_model.save_model(model_path+ \".pth.tar\")\n\n\n\n    # Evaluating model\n    def eval_model(model):\n        pred_series = model.predict(n=26, future_covariates=covariates)\n        mape1 = mape(pred_series, val_transformed)\n        mse1 = mse(pred_series, val_transformed)\n        rmse1 = rmse(pred_series, val_transformed)\n        print(\"MAPE: {:.2f}%\".format(mape1))\n        print(\"MSE: {:.2f}%\".format(mse1))\n        print(\"RMSE: {:.2f}%\".format(rmse1))\n        metrics.log_metric(\"MAPE\",\"{:.2f}%\".format(mape1))\n        metrics.log_metric(\"MSE\", \"{}\".format(mse1))\n        metrics.log_metric(\"RMSE\", \"{}\".format(rmse1))\n\n    eval_model(my_model)\n    example_output = namedtuple('ExampleOutputs', ['tar_path'])\n    return example_output(path)\n\n"
            ],
            "image": "gcr.io/impact-analytics-sandbox/create_base_image:v1"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "sequential-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "sequential-model-2-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "sequential-model-2"
                }
              ]
            },
            "sequential-model-3-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "sequential-model-3"
                }
              ]
            },
            "sequential-model-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "sequential-model"
                }
              ]
            }
          }
        },
        "tasks": {
          "deploy-to-run": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-deploy-to-run"
            },
            "dependentTasks": [
              "sequential-model"
            ],
            "inputs": {
              "parameters": {
                "tar_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "tar_path",
                    "producerTask": "sequential-model"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "deploy-to-run"
            }
          },
          "deploy-to-run-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-deploy-to-run-2"
            },
            "dependentTasks": [
              "sequential-model-2"
            ],
            "inputs": {
              "parameters": {
                "tar_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "tar_path",
                    "producerTask": "sequential-model-2"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "deploy-to-run-2"
            }
          },
          "deploy-to-run-3": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-deploy-to-run-3"
            },
            "dependentTasks": [
              "sequential-model-3"
            ],
            "inputs": {
              "parameters": {
                "tar_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "tar_path",
                    "producerTask": "sequential-model-3"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "deploy-to-run-3"
            }
          },
          "get-air-data": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-get-air-data"
            },
            "taskInfo": {
              "name": "get-air-data"
            }
          },
          "sequential-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-sequential-model"
            },
            "dependentTasks": [
              "get-air-data"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_data_path",
                    "producerTask": "get-air-data"
                  }
                }
              },
              "parameters": {
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "LSTM"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "sequential-model"
            }
          },
          "sequential-model-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-sequential-model-2"
            },
            "dependentTasks": [
              "get-air-data",
              "sequential-model"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_data_path",
                    "producerTask": "get-air-data"
                  }
                }
              },
              "parameters": {
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "GRU"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "sequential-model-2"
            }
          },
          "sequential-model-3": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-sequential-model-3"
            },
            "dependentTasks": [
              "get-air-data",
              "sequential-model-2"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_data_path",
                    "producerTask": "get-air-data"
                  }
                }
              },
              "parameters": {
                "model_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "RNN"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "sequential-model-3"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "api_endpoint": {
            "type": "STRING"
          },
          "display_name": {
            "type": "STRING"
          },
          "project": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          },
          "serving_container_image_uri": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "sequential-model-2-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "sequential-model-3-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "sequential-model-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.9"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://impact-analytics-sandbox-bucket/pipeline_root_air/",
    "parameters": {
      "api_endpoint": {
        "stringValue": "us-central1-aiplatform.googleapis.com"
      },
      "display_name": {
        "stringValue": "air-job20220301111825"
      },
      "project": {
        "stringValue": "impact-analytics-sandbox"
      },
      "region": {
        "stringValue": "us-central1"
      },
      "serving_container_image_uri": {
        "stringValue": "gcr.io/impact-analytics-sandbox/base_container:v3"
      }
    }
  }
}