{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eaec34c-9ec7-4377-9f84-39297d850891",
   "metadata": {},
   "source": [
    "# Vertex AI Pipelines Sandbox Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0e984-e3eb-49ba-a30b-a0bd777dd628",
   "metadata": {},
   "source": [
    "The purpose of this demo is to demonstrate creation of kubeflow ML pipelines for custom models\n",
    "\n",
    "Download data from - https://github.com/jbrownlee/Datasets/blob/master/airline-passengers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7141f-abff-4ae2-b80f-fe6b6ab86589",
   "metadata": {},
   "source": [
    "The purpose of this demo is to demonstrate creation of kubeflow ML pipelines for custom models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5338e4-6b34-448a-b317-7a3b290de33e",
   "metadata": {},
   "source": [
    "## Step 1 : Build and Push Base Container image\n",
    "We have created a base container which have all dependencies installed. Since, all dependencies are installed in the base container, we do not have to install the dependencies for every custom component repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6112db7-ed8c-45e4-afb6-80f0f1c57109",
   "metadata": {},
   "source": [
    "Build and Push container to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c4f9c-7119-42b8-a273-6942f84deab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMAGE_URI=\"gcr.io/impact-analytics-sandbox/base_container:v3\"\n",
    "PROJECT_ID=\"applied-ai-practice00\"\n",
    "IMAGE_NAME=\"mlops_base_container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d53075-0845-4179-b59d-105cd30d6413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to python Analytics_Pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9826fc-8000-44ad-aa59-33c7843952bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd base_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b12b03-4ee5-470d-8b7e-b3a03396ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t $IMAGE_NAME:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971b152-9584-467d-a65b-5cf81afe43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker us-central1-docker.pkg.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376ee2b-3c50-4738-a429-e6fb25a6484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create mlops-pipelines-container-repository --location=us-central1  --repository-format=docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ceab2c-42ed-4743-8590-3ab1332e4891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker tag $IMAGE_NAME:latest us-central1-docker.pkg.dev/applied-ai-practice00/mlops-pipelines-container-repository/mlops_base_container:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf3807-0bed-4510-a389-608e5f345452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker push us-central1-docker.pkg.dev/$PROJECT_ID/mlops-pipelines-container-repository/mlops_base_container:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc63063-ba71-45cd-a537-1ea9921679c4",
   "metadata": {},
   "source": [
    "## Step 1: Installing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d698918-ec97-4b49-90cd-b60f198ca18d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade \\\n",
    "google-cloud-aiplatform \\\n",
    "kfp \\\n",
    "google-cloud-pipeline-components \\\n",
    "google-cloud-bigquery \\\n",
    "pandas \\\n",
    "numpy \\\n",
    "torch \\\n",
    "matplotlib \\\n",
    "darts \\\n",
    "github3.py \\\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bf371-705c-460f-a493-03697d5f1e99",
   "metadata": {},
   "source": [
    "## Step 2: Upload your Dataset to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9135c6-e258-468c-b604-1c370dcc190f",
   "metadata": {},
   "source": [
    "## Step 3: Defining the pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb247cda-e00e-4b33-919e-3f961c2b71d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_op_from_component\n",
    "from google.cloud import aiplatform\n",
    "# from google.cloud.aiplatform import CustomContainerTrainingJob\n",
    "from typing import NamedTuple\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d50ee3-39af-44c5-8eb7-337fe7ea3120",
   "metadata": {},
   "source": [
    "Since we are training a custom model using DARTS package we have created custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8bc1e-ed33-41ea-a0c9-174249ef4c06",
   "metadata": {},
   "source": [
    "### a) Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83810c2c-561f-4139-959f-093e87716dcd",
   "metadata": {},
   "source": [
    "Below component is used to fetch data from bigquery and its output will be passed as input to the training component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4012151-4a48-4c19-b88e-ffd8ebf1a0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom component to fetch data from BigQuery\n",
    "@dsl.component(\n",
    "    # base_image=\"us-central1-docker.pkg.dev/applied-ai-practice00/mlops-pipelines-container-repository/mlops_base_container:latest\",\n",
    "    # output_component_file=\"create_dataset.yaml\"\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"google-cloud-bigquery\", \"db_dtypes\"],\n",
    ")\n",
    "def get_air_data(\n",
    "    #bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    bqclient = bigquery.Client(project=\"applied-ai-practice00\")\n",
    "\n",
    "    # Download query results.\n",
    "    query_string = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `applied-ai-practice00.mlops_dataset.airline_passengers`\n",
    "    \"\"\"\n",
    "    # get dataframe by querying bigquery table\n",
    "    air_df = (\n",
    "        bqclient.query(query_string)\n",
    "            .result()\n",
    "            .to_dataframe(\n",
    "            create_bqstorage_client=True,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    air_df.to_csv(output_data_path,index=False)\n",
    "    print(output_data_path)\n",
    "    \n",
    "compiler.Compiler().compile(pipeline_func=get_air_data, package_path=\"get_air_data.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f08d4-f824-4e5c-a84f-780bebb51654",
   "metadata": {},
   "source": [
    "### b) Training and Evaluation of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443db165-3134-4276-9500-9fc35b3811f8",
   "metadata": {},
   "source": [
    "Sequential_model component is used for model training and evaluation. We are transforming data, creating and saving the darts model based on model type parameter and evaluate model based on MAPE, MSE, and RMSE metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e495500-bb09-4416-bb48-4f7ba386d77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom component for Model training and evaluation\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"torch\", \"matplotlib\", \"darts\", \"google-cloud-storage\"]\n",
    ")\n",
    "def sequential_model(\n",
    "    dataset:  Input[Dataset],\n",
    "    model_type: str,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    model_path: OutputPath(\"Model\"),\n",
    ") -> NamedTuple('ExampleOutputs', [('tar_path', str)]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    from darts import TimeSeries\n",
    "    from darts.utils.timeseries_generation import (\n",
    "        gaussian_timeseries,\n",
    "        linear_timeseries,\n",
    "        sine_timeseries,\n",
    "    )\n",
    "    from darts.models import RNNModel\n",
    "    from darts.metrics import mape, mse, rmse\n",
    "    from darts.dataprocessing.transformers import Scaler\n",
    "    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "    from google.cloud import storage\n",
    "    import glob\n",
    "    import shutil\n",
    "    from collections import namedtuple\n",
    "    from typing import NamedTuple\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "\n",
    "        print(\n",
    "            \"File {} uploaded to {}.\".format(\n",
    "                source_file_name, destination_blob_name\n",
    "            )\n",
    "        )    \n",
    "    # Data Preparation\n",
    "    air_df = pd.read_csv(dataset.path)\n",
    "    air_df['Month']=pd.to_datetime(air_df['Month'])\n",
    "    air_df.sort_values(by=\"Month\",inplace=True)\n",
    "    air_df.index = air_df['Month']\n",
    "    air_df.drop(\"Month\",inplace=True,axis=1)\n",
    "    series = TimeSeries.from_dataframe(air_df)\n",
    "    # Create training and validation sets:\n",
    "    train, val = series.split_after(pd.Timestamp(\"19590101\"))\n",
    "\n",
    "    # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
    "    transformer = Scaler()\n",
    "    train_transformed = transformer.fit_transform(train)\n",
    "    val_transformed = transformer.transform(val)\n",
    "    series_transformed = transformer.transform(series)\n",
    "\n",
    "    # create month and year covariate series\n",
    "    year_series = datetime_attribute_timeseries(\n",
    "        pd.date_range(start=series.start_time(), freq=series.freq_str, periods=1000),\n",
    "        attribute=\"year\",\n",
    "        one_hot=False,\n",
    "    )\n",
    "    year_series = Scaler().fit_transform(year_series)\n",
    "    month_series = datetime_attribute_timeseries(\n",
    "        year_series, attribute=\"month\", one_hot=True\n",
    "    )\n",
    "    covariates = year_series.stack(month_series)\n",
    "    cov_train, cov_val = covariates.split_after(pd.Timestamp(\"19590101\"))\n",
    "    \n",
    "    \n",
    "    #setting hyperparameters\n",
    "    hidden_dim=20\n",
    "    dropout=0\n",
    "    batch_size=16\n",
    "    epochs=300\n",
    "    learning_rate=1e-3\n",
    "    optimizer_kwargs={\"lr\":learning_rate }\n",
    "    model_name=\"Air_\"+model_type\n",
    "    log_tensorboard=True\n",
    "    random_state=42\n",
    "    training_length=20\n",
    "    input_chunk_length=14\n",
    "    force_reset=True\n",
    "    save_checkpoints=True\n",
    "    \n",
    "    # Model Creation\n",
    "    my_model = RNNModel(\n",
    "        model=model_type,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=epochs,\n",
    "        optimizer_kwargs=optimizer_kwargs,\n",
    "        model_name=model_name,\n",
    "        log_tensorboard=log_tensorboard,\n",
    "        random_state=random_state,\n",
    "        training_length=training_length,\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "    )\n",
    "    \n",
    "    my_model.fit(\n",
    "        train_transformed,\n",
    "        future_covariates=covariates,\n",
    "        val_series=val_transformed,\n",
    "        val_future_covariates=covariates,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    # metadata about model\n",
    "    model.metadata[\"hidden_dim\"] = hidden_dim\n",
    "    model.metadata[\"dropout\"] = dropout\n",
    "    model.metadata[\"batch_size\"] = batch_size\n",
    "    model.metadata[\"n_epochs\"]=  epochs\n",
    "    model.metadata[\"learning rate\"] = learning_rate\n",
    "    #model.metadata[\"model_name\"]=\"Air_\"+model_type\n",
    "    \n",
    "    model.metadata[\"random_state\"] = random_state\n",
    "    model.metadata[\"training_length\"] = training_length\n",
    "    model.metadata[\"input_chunk_length\"] = input_chunk_length\n",
    "     \n",
    "    from datetime import datetime\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    k = glob.glob('darts_logs' + \"/**\", recursive=True)\n",
    "    blobs_list = []\n",
    "    for i in k:\n",
    "        if '.' in i:\n",
    "            blobs_list.append(i)\n",
    "\n",
    "    bucket_name = 'applied-ai-practice00-bucket'\n",
    "    \n",
    "    # Saving ML models to bucket\n",
    "    for blob in blobs_list:\n",
    "        print(blob)    \n",
    "        source_file_name = blob \n",
    "        destination_blob_name = 'model_logs{}/'.format(TIMESTAMP)+blob\n",
    "        upload_blob(bucket_name, source_file_name, destination_blob_name)\n",
    "        if blob.endswith(\".pth.tar\"):\n",
    "            path = bucket_name + \"/\" + destination_blob_name\n",
    "\n",
    "    my_model.save(model_path+ \".pth.tar\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Evaluating model\n",
    "    def eval_model(model):\n",
    "        pred_series = model.predict(n=26, future_covariates=covariates)\n",
    "        mape1 = mape(pred_series, val_transformed)\n",
    "        mse1 = mse(pred_series, val_transformed)\n",
    "        rmse1 = rmse(pred_series, val_transformed)\n",
    "        print(\"MAPE: {:.2f}%\".format(mape1))\n",
    "        print(\"MSE: {:.2f}%\".format(mse1))\n",
    "        print(\"RMSE: {:.2f}%\".format(rmse1))\n",
    "        metrics.log_metric(\"MAPE\",\"{:.2f}%\".format(mape1))\n",
    "        metrics.log_metric(\"MSE\", \"{}\".format(mse1))\n",
    "        metrics.log_metric(\"RMSE\", \"{}\".format(rmse1))\n",
    "    \n",
    "    eval_model(my_model)\n",
    "    example_output = namedtuple('ExampleOutputs', ['tar_path'])\n",
    "    return example_output(path)\n",
    "    \n",
    "    \n",
    "compiler.Compiler().compile(pipeline_func=sequential_model, package_path=\"sequential_model.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76562456-b7f4-4856-8242-289c465b7d62",
   "metadata": {},
   "source": [
    "### c) Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924788ca-d40b-4f19-9824-1725fa42ef37",
   "metadata": {},
   "source": [
    "deploy_to_run component is for deploying model. It takes the model weight path as an input and pushes the updated weights to a git repo which contains the inference/helper code to make predictions. As soon as the the file is pushed, a Cloud Build is triggered which deploys the code on cloud run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3ecce-ae3a-45a8-8545-d709a1df7659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"github3.py\"],  \n",
    "    )\n",
    "def deploy_to_run(\n",
    "    tar_path: str,\n",
    "    branch: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "    ):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    from github3 import GitHub\n",
    " \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(tar_path.split(\"/\")[0])\n",
    "    blob = bucket.blob(\"/\".join(tar_path.split(\"/\")[1:]))\n",
    "    blob.download_to_filename(\"/tmp/model.pth.tar\")\n",
    "\n",
    "    g = GitHub(username=\"munadkatSearce\", token=\"your_access_token\")\n",
    "\n",
    "    repo = g.repository(\"munadkatSearce\", \"darts-model-serving\")\n",
    "    contents = repo.file_contents(\"model.pth.tar\", ref=branch)\n",
    "\n",
    "    model_file = open(\"/tmp/model.pth.tar\", \"rb\")\n",
    "    file_content=model_file.read()\n",
    "    model_file.close()\n",
    "\n",
    "    # repo.upd(path=contents.path, message=\"new_model_trained\", content=file_content, sha=contents.sha)\n",
    "    contents.update(message=\"new_model_trained\", content=file_content)    \n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=deploy_to_run, package_path=\"deploy_to_run.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28ad61-e664-4d4c-86fb-60273a2e1c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'air-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea9e28-c42d-451b-aec6-22167bc6aa94",
   "metadata": {},
   "source": [
    "We are setting the global variables to pass in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cc693-a2a0-429a-94f7-46d4858cf7c3",
   "metadata": {},
   "source": [
    "## Step 3: Defining the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1cd10-c9f0-4e8a-8fd9-d833dbe28cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting Global variables\n",
    "PROJECT_ID=\"applied-ai-practice00\"\n",
    "REGION = \"us-central1\"\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://\"+PROJECT_ID+\"-bucket\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root_air/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f8529-150d-4681-b8c9-1776ac5e883f",
   "metadata": {},
   "source": [
    "Below is the code to define ML pipeline. The pipeline first fetches data from bigquery source, it then trains 3 models sequentially and deploys them to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02445af6-2781-4a67-812c-4a3397fd3ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining pipeline\n",
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"sequential-pipeline\",\n",
    "    \n",
    ")\n",
    "def pipeline(\n",
    "    #url: str = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "    #thresholds_dict_str: str = '{\"roc\":0.8}',\n",
    "    # serving_container_image_uri: str = f\"us-central1-docker.pkg.dev/applied-ai-practice00/mlops-pipelines-container-repository/mlops_base_container:latest\"\n",
    "    ):\n",
    "    \n",
    "    # fetch data\n",
    "    data_op = get_air_data()\n",
    "    \n",
    "    # train and evaluate mulitple models\n",
    "    train_model_op_lstm = sequential_model(model_type=\"LSTM\",dataset= data_op.output)\n",
    "    train_model_op_GRU = sequential_model(model_type=\"GRU\", dataset=data_op.output) #.after(train_model_op_lstm)\n",
    "    train_model_op_RNN = sequential_model(model_type=\"RNN\", dataset=data_op.output) #.after(train_model_op_GRU)\n",
    "    \n",
    "    # deploy models\n",
    "    \n",
    "    deploy_task1 = deploy_to_run(\n",
    "    tar_path=train_model_op_lstm.outputs[\"tar_path\"], branch=\"main\"\n",
    "    )\n",
    "    \n",
    "    deploy_task2 = deploy_to_run(\n",
    "    tar_path=train_model_op_GRU.outputs[\"tar_path\"], branch=\"GRU\"\n",
    "    )\n",
    "    \n",
    "    deploy_task3 = deploy_to_run(\n",
    "    tar_path=train_model_op_RNN.outputs[\"tar_path\"], branch=\"RNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7ddf7-a19b-4073-9b61-5ed3dd6ed2f7",
   "metadata": {},
   "source": [
    "Pipeline compiler will compile the pipeline and store the pipeline configuration inside a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a636f01-7f2b-4e5a-a54c-625adb3dadd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"custom_train_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cc2fc-6741-41b7-8994-7a8d686bbe7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81122313-6fa7-4a07-a30e-e970e707fffd",
   "metadata": {},
   "source": [
    "## Step 4: Running the Pipeline Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee7356-bf67-4d10-8c86-aa134daab98d",
   "metadata": {},
   "source": [
    "Here we are deploying pipeline job which will be submitted for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a5d51-2bdb-4c12-93c7-a786a8be67eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"custom-train-pipeline-1\",\n",
    "    template_path=\"custom_train_pipeline.json\",\n",
    "    job_id=\"custom-train-pipeline-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a3308-1153-45ce-9d4c-06200b58a4c8",
   "metadata": {},
   "source": [
    "Below code will submit job to create the pipeline, you can use the link at the bottom to view pipeline status. Below link will help us monitor the execution of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a2ddb-7a8d-4a97-b8a1-ecfb4aa3b97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submit pipeline job\n",
    "pipeline_job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
