{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eaec34c-9ec7-4377-9f84-39297d850891",
   "metadata": {},
   "source": [
    "# Impact Analytics Sandbox Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0e984-e3eb-49ba-a30b-a0bd777dd628",
   "metadata": {},
   "source": [
    "The purpose of this demo is to demonstrate creation of kubeflow ML pipelines for Impact analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5338e4-6b34-448a-b317-7a3b290de33e",
   "metadata": {},
   "source": [
    "## Step 1 : Build and Push Base Container image\n",
    "We have created a base container which have all dependencies installed. Since, all dependencies are installed in the base container, we do not have to install the dependencies for every custom component repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6112db7-ed8c-45e4-afb6-80f0f1c57109",
   "metadata": {},
   "source": [
    "Build and Push container to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8c4f9c-7119-42b8-a273-6942f84deab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMAGE_URI=\"gcr.io/impact-analytics-sandbox/base_container:v3\"\n",
    "PROJECT_ID=\"applied-ai-practice00\"\n",
    "IMAGE_NAME=\"mlops_base_container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d2e7fa-a8ae-4b39-961b-531b803976f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Analytics_Pipeline.ipynb to script\n",
      "[NbConvertApp] Writing 13660 bytes to Analytics_Pipeline.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Analytics_Pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9826fc-8000-44ad-aa59-33c7843952bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd base_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b12b03-4ee5-470d-8b7e-b3a03396ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t $IMAGE_NAME:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971b152-9584-467d-a65b-5cf81afe43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker us-central1-docker.pkg.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376ee2b-3c50-4738-a429-e6fb25a6484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create mlops-pipelines-container-repository --location=us-central1  --repository-format=docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ceab2c-42ed-4743-8590-3ab1332e4891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker tag $IMAGE_NAME:latest us-central1-docker.pkg.dev/applied-ai-practice00/mlops-pipelines-container-repository/mlops_base_container:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebf3807-0bed-4510-a389-608e5f345452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker push us-central1-docker.pkg.dev/$PROJECT_ID/mlops-pipelines-container-repository/mlops_base_container:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc63063-ba71-45cd-a537-1ea9921679c4",
   "metadata": {},
   "source": [
    "## Step 1: Installing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d698918-ec97-4b49-90cd-b60f198ca18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.51.0)\n",
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components in /opt/conda/lib/python3.10/site-packages (2.14.1)\n",
      "Requirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.10/site-packages (3.22.0)\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.23.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.8.4)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting darts\n",
      "  Downloading darts-0.29.0-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: github3.py in /opt/conda/lib/python3.10/site-packages (4.0.1)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.14.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.25.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.7.1)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (8.1.7)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (26.1.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.26.18)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components) (3.1.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.9.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch)\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Collecting holidays>=0.11.1 (from darts)\n",
      "  Downloading holidays-0.48-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: joblib>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from darts) (1.4.2)\n",
      "Collecting nfoursid>=1.0.0 (from darts)\n",
      "  Downloading nfoursid-1.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pmdarima>=1.8.0 (from darts)\n",
      "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
      "Collecting pyod>=0.9.5 (from darts)\n",
      "  Downloading pyod-1.1.3.tar.gz (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from darts) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from darts) (1.13.0)\n",
      "Collecting shap>=0.40.0 (from darts)\n",
      "  Downloading shap-0.45.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Collecting statsforecast>=1.4 (from darts)\n",
      "  Downloading statsforecast-1.7.4-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from darts) (0.14.2)\n",
      "Collecting tbats>=1.1.0 (from darts)\n",
      "  Downloading tbats-1.1.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /opt/conda/lib/python3.10/site-packages (from darts) (4.66.4)\n",
      "Collecting xarray>=0.17.0 (from darts)\n",
      "  Downloading xarray-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting xgboost>=1.6.0 (from darts)\n",
      "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting pytorch-lightning>=1.5.0 (from darts)\n",
      "  Downloading pytorch_lightning-2.2.4-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tensorboardX>=2.1 in /opt/conda/lib/python3.10/site-packages (from darts) (2.6.2.2)\n",
      "Requirement already satisfied: pyjwt>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.3.0->github3.py) (2.8.0)\n",
      "Requirement already satisfied: uritemplate>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from github3.py) (3.0.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (2.1.5)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2024.2.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (69.5.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (2.0.0)\n",
      "Collecting Cython!=0.29.18,!=0.29.31,>=0.29 (from pmdarima>=1.8.0->darts)\n",
      "  Downloading Cython-3.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.18.2)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.3.0->github3.py) (42.0.7)\n",
      "Requirement already satisfied: numba>=0.51 in /opt/conda/lib/python3.10/site-packages (from pyod>=0.9.5->darts) (0.59.1)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning>=1.5.0->darts)\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning>=1.5.0->darts)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.1->darts) (3.5.0)\n",
      "Collecting slicer==0.0.8 (from shap>=0.40.0->darts)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap>=0.40.0->darts) (3.0.0)\n",
      "Collecting coreforecast>=0.0.7 (from statsforecast>=1.4->darts)\n",
      "  Downloading coreforecast-0.0.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting fugue>=0.8.1 (from statsforecast>=1.4->darts)\n",
      "  Downloading fugue-0.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting utilsforecast>=0.0.24 (from statsforecast>=1.4->darts)\n",
      "  Downloading utilsforecast-0.1.10-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.14.0->darts) (0.5.6)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.3.0->github3.py) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (3.9.5)\n",
      "Collecting triad>=0.9.6 (from fugue>=0.8.1->statsforecast>=1.4->darts)\n",
      "  Downloading triad-0.9.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast>=1.4->darts)\n",
      "  Downloading adagio-0.2.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.42.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (4.0.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.3.0->github3.py) (2.22)\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from triad>=0.9.6->fugue>=0.8.1->statsforecast>=1.4->darts) (16.1.0)\n",
      "Collecting fs (from triad>=0.9.6->fugue>=0.8.1->statsforecast>=1.4->darts)\n",
      "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting appdirs~=1.4.3 (from fs->triad>=0.9.6->fugue>=0.8.1->statsforecast>=1.4->darts)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Downloading google_cloud_bigquery-3.23.0-py2.py3-none-any.whl (237 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.1/237.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading darts-0.29.0-py3-none-any.whl (884 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.7/884.7 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading holidays-0.48-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nfoursid-1.0.1-py3-none-any.whl (16 kB)\n",
      "Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.2.4-py3-none-any.whl (802 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shap-0.45.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading statsforecast-1.7.4-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.8/121.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2024.5.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading coreforecast-0.0.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Cython-3.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fugue-0.9.0-py3-none-any.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading utilsforecast-0.1.10-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading adagio-0.2.4-py3-none-any.whl (26 kB)\n",
      "Downloading triad-0.9.6-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Building wheels for collected packages: pyod\n",
      "  Building wheel for pyod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyod: filename=pyod-1.1.3-py3-none-any.whl size=190250 sha256=c47bbecb1c9b763391bd6a6d9f0ee5adfc262d804c36539a6911d596db1e6896\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/05/f8/db/124d43bec122d6ec0ab3713fadfe25ebed8af52ec561682b4e\n",
      "Successfully built pyod\n",
      "Installing collected packages: mpmath, appdirs, triton, sympy, slicer, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, fs, Cython, coreforecast, xgboost, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, holidays, xarray, utilsforecast, triad, shap, pyod, nvidia-cusolver-cu12, nfoursid, torch, pmdarima, adagio, torchmetrics, tbats, google-cloud-storage, google-cloud-bigquery, fugue, statsforecast, pytorch-lightning, darts\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.4\n",
      "    Uninstalling matplotlib-3.8.4:\n",
      "      Successfully uninstalled matplotlib-3.8.4\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.14.0\n",
      "    Uninstalling google-cloud-storage-2.14.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.14.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.22.0\n",
      "    Uninstalling google-cloud-bigquery-3.22.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.22.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.8.3 requires matplotlib<3.9,>=3.2, but you have matplotlib 3.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Cython-3.0.10 adagio-0.2.4 appdirs-1.4.4 coreforecast-0.0.9 darts-0.29.0 fs-2.4.16 fugue-0.9.0 google-cloud-bigquery-3.23.0 google-cloud-storage-2.16.0 holidays-0.48 lightning-utilities-0.11.2 matplotlib-3.9.0 mpmath-1.3.0 nfoursid-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pmdarima-2.0.4 pyod-1.1.3 pytorch-lightning-2.2.4 shap-0.45.1 slicer-0.0.8 statsforecast-1.7.4 sympy-1.12 tbats-1.1.3 torch-2.3.0 torchmetrics-1.4.0.post0 triad-0.9.6 triton-2.3.0 utilsforecast-0.1.10 xarray-2024.5.0 xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade \\\n",
    "google-cloud-aiplatform \\\n",
    "kfp \\\n",
    "google-cloud-pipeline-components \\\n",
    "google-cloud-bigquery \\\n",
    "pandas \\\n",
    "numpy \\\n",
    "torch \\\n",
    "matplotlib \\\n",
    "darts \\\n",
    "github3.py \\\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bf371-705c-460f-a493-03697d5f1e99",
   "metadata": {},
   "source": [
    "## Step 2: Upload your Dataset to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9135c6-e258-468c-b604-1c370dcc190f",
   "metadata": {},
   "source": [
    "## Step 3: Defining the pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb247cda-e00e-4b33-919e-3f961c2b71d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_op_from_component\n",
    "from google.cloud import aiplatform\n",
    "# from google.cloud.aiplatform import CustomContainerTrainingJob\n",
    "from typing import NamedTuple\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d50ee3-39af-44c5-8eb7-337fe7ea3120",
   "metadata": {},
   "source": [
    "Since we are training a custom model using DARTS package we have created custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8bc1e-ed33-41ea-a0c9-174249ef4c06",
   "metadata": {},
   "source": [
    "### a) Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83810c2c-561f-4139-959f-093e87716dcd",
   "metadata": {},
   "source": [
    "Below component is used to fetch data from bigquery and its output will be passed as input to the training component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4012151-4a48-4c19-b88e-ffd8ebf1a0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom component to fetch data from BigQuery\n",
    "@component(\n",
    "    # base_image=\"us-central1-docker.pkg.dev/applied-ai-practice00/mlops-pipelines-container-repository/mlops_base_container:latest\",\n",
    "    # output_component_file=\"create_dataset.yaml\"\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"torch\", \"matplotlib\", \"darts\", \"google-cloud-storage\"],  \n",
    ")\n",
    "def get_air_data(\n",
    "    #bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    bqclient = bigquery.Client(project=\"impact-analytics-sandbox\")\n",
    "\n",
    "    # Download query results.\n",
    "    query_string = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `applied-ai-practice00.mlops_dataset.airline_passengers`\n",
    "    \"\"\"\n",
    "    # get dataframe by querying bigquery table\n",
    "    air_df = (\n",
    "        bqclient.query(query_string)\n",
    "            .result()\n",
    "            .to_dataframe(\n",
    "            create_bqstorage_client=True,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    air_df.to_csv(output_data_path,index=False)\n",
    "    print(output_data_path)\n",
    "    \n",
    "compiler.Compiler().compile(pipeline_func=get_air_data, package_path=\"get_air_data.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f08d4-f824-4e5c-a84f-780bebb51654",
   "metadata": {},
   "source": [
    "### b) Training and Evaluation of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443db165-3134-4276-9500-9fc35b3811f8",
   "metadata": {},
   "source": [
    "Sequential_model component is used for model training and evaluation. We are transforming data, creating and saving the darts model based on model type parameter and evaluate model based on MAPE, MSE, and RMSE metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e495500-bb09-4416-bb48-4f7ba386d77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom component for Model training and evaluation\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"torch\", \"matplotlib\", \"darts\", \"google-cloud-storage\"]\n",
    ")\n",
    "def sequential_model(\n",
    "    dataset:  Input[Dataset],\n",
    "    model_type: str,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    model_path: OutputPath(\"Model\"),\n",
    ") -> NamedTuple('ExampleOutputs', [('tar_path', str)]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    from darts import TimeSeries\n",
    "    from darts.utils.timeseries_generation import (\n",
    "        gaussian_timeseries,\n",
    "        linear_timeseries,\n",
    "        sine_timeseries,\n",
    "    )\n",
    "    from darts.models import RNNModel\n",
    "    from darts.metrics import mape, mse, rmse\n",
    "    from darts.dataprocessing.transformers import Scaler\n",
    "    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "    from google.cloud import storage\n",
    "    import glob\n",
    "    import shutil\n",
    "    from collections import namedtuple\n",
    "    from typing import NamedTuple\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "\n",
    "        print(\n",
    "            \"File {} uploaded to {}.\".format(\n",
    "                source_file_name, destination_blob_name\n",
    "            )\n",
    "        )    \n",
    "    # Data Preparation\n",
    "    air_df = pd.read_csv(dataset.path)\n",
    "    air_df['Month']=pd.to_datetime(air_df['Month'])\n",
    "    air_df.sort_values(by=\"Month\",inplace=True)\n",
    "    air_df.index = air_df['Month']\n",
    "    air_df.drop(\"Month\",inplace=True,axis=1)\n",
    "    series = TimeSeries.from_dataframe(air_df)\n",
    "    # Create training and validation sets:\n",
    "    train, val = series.split_after(pd.Timestamp(\"19590101\"))\n",
    "\n",
    "    # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
    "    transformer = Scaler()\n",
    "    train_transformed = transformer.fit_transform(train)\n",
    "    val_transformed = transformer.transform(val)\n",
    "    series_transformed = transformer.transform(series)\n",
    "\n",
    "    # create month and year covariate series\n",
    "    year_series = datetime_attribute_timeseries(\n",
    "        pd.date_range(start=series.start_time(), freq=series.freq_str, periods=1000),\n",
    "        attribute=\"year\",\n",
    "        one_hot=False,\n",
    "    )\n",
    "    year_series = Scaler().fit_transform(year_series)\n",
    "    month_series = datetime_attribute_timeseries(\n",
    "        year_series, attribute=\"month\", one_hot=True\n",
    "    )\n",
    "    covariates = year_series.stack(month_series)\n",
    "    cov_train, cov_val = covariates.split_after(pd.Timestamp(\"19590101\"))\n",
    "    \n",
    "    \n",
    "    #setting hyperparameters\n",
    "    hidden_dim=20\n",
    "    dropout=0\n",
    "    batch_size=16\n",
    "    epochs=300\n",
    "    learning_rate=1e-3\n",
    "    optimizer_kwargs={\"lr\":learning_rate }\n",
    "    model_name=\"Air_\"+model_type\n",
    "    log_tensorboard=True\n",
    "    random_state=42\n",
    "    training_length=20\n",
    "    input_chunk_length=14\n",
    "    force_reset=True\n",
    "    save_checkpoints=True\n",
    "    \n",
    "    # Model Creation\n",
    "    my_model = RNNModel(\n",
    "        model=model_type,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=epochs,\n",
    "        optimizer_kwargs=optimizer_kwargs,\n",
    "        model_name=model_name,\n",
    "        log_tensorboard=log_tensorboard,\n",
    "        random_state=random_state,\n",
    "        training_length=training_length,\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "    )\n",
    "    \n",
    "    my_model.fit(\n",
    "        train_transformed,\n",
    "        future_covariates=covariates,\n",
    "        val_series=val_transformed,\n",
    "        val_future_covariates=covariates,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    # metadata about model\n",
    "    model.metadata[\"hidden_dim\"] = hidden_dim\n",
    "    model.metadata[\"dropout\"] = dropout\n",
    "    model.metadata[\"batch_size\"] = batch_size\n",
    "    model.metadata[\"n_epochs\"]=  epochs\n",
    "    model.metadata[\"learning rate\"] = learning_rate\n",
    "    #model.metadata[\"model_name\"]=\"Air_\"+model_type\n",
    "    \n",
    "    model.metadata[\"random_state\"] = random_state\n",
    "    model.metadata[\"training_length\"] = training_length\n",
    "    model.metadata[\"input_chunk_length\"] = input_chunk_length\n",
    "     \n",
    "    from datetime import datetime\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    k = glob.glob('darts_logs' + \"/**\", recursive=True)\n",
    "    blobs_list = []\n",
    "    for i in k:\n",
    "        if '.' in i:\n",
    "            blobs_list.append(i)\n",
    "\n",
    "    bucket_name = 'impact-analytics-experiments-bucket01'\n",
    "    \n",
    "    # Saving ML models to bucket\n",
    "    for blob in blobs_list:\n",
    "        print(blob)    \n",
    "        source_file_name = blob \n",
    "        destination_blob_name = 'model_logs{}/'.format(TIMESTAMP)+blob\n",
    "        upload_blob(bucket_name, source_file_name, destination_blob_name)\n",
    "        if blob.endswith(\".pth.tar\"):\n",
    "            path = bucket_name + \"/\" + destination_blob_name\n",
    "\n",
    "    my_model.save_model(model_path+ \".pth.tar\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Evaluating model\n",
    "    def eval_model(model):\n",
    "        pred_series = model.predict(n=26, future_covariates=covariates)\n",
    "        mape1 = mape(pred_series, val_transformed)\n",
    "        mse1 = mse(pred_series, val_transformed)\n",
    "        rmse1 = rmse(pred_series, val_transformed)\n",
    "        print(\"MAPE: {:.2f}%\".format(mape1))\n",
    "        print(\"MSE: {:.2f}%\".format(mse1))\n",
    "        print(\"RMSE: {:.2f}%\".format(rmse1))\n",
    "        metrics.log_metric(\"MAPE\",\"{:.2f}%\".format(mape1))\n",
    "        metrics.log_metric(\"MSE\", \"{}\".format(mse1))\n",
    "        metrics.log_metric(\"RMSE\", \"{}\".format(rmse1))\n",
    "    \n",
    "    eval_model(my_model)\n",
    "    example_output = namedtuple('ExampleOutputs', ['tar_path'])\n",
    "    return example_output(path)\n",
    "    \n",
    "    \n",
    "compiler.Compiler().compile(pipeline_func=sequential_model, package_path=\"sequential_model.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76562456-b7f4-4856-8242-289c465b7d62",
   "metadata": {},
   "source": [
    "### c) Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924788ca-d40b-4f19-9824-1725fa42ef37",
   "metadata": {},
   "source": [
    "deploy_to_run component is for deploying model. It takes the model weight path as an input and pushes the updated weights to a git repo which contains the inference/helper code to make predictions. As soon as the the file is pushed, a Cloud Build is triggered which deploys the code on cloud run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81e3ecce-ae3a-45a8-8545-d709a1df7659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"github3.py\"],  \n",
    "    )\n",
    "def deploy_to_run(\n",
    "    tar_path: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "    ):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    from github import Github\n",
    "\n",
    "   \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(tar_path.split(\"/\")[0])\n",
    "    blob = bucket.blob(\"/\".join(tar_path.split(\"/\")[1:]))\n",
    "    blob.download_to_filename(\"/tmp/model.pth.tar\")\n",
    "\n",
    "    g = Github(\"your_github_secret\")\n",
    "\n",
    "    repo = g.get_repo(\"munadkatSearce/darts-model-serving\")\n",
    "    contents = repo.get_contents(\"model.pth.tar\", ref=\"main\")\n",
    "\n",
    "    model_file = open(\"/tmp/model.pth.tar\", \"rb\")\n",
    "    file_content=model_file.read()\n",
    "    model_file.close()\n",
    "\n",
    "    repo.update_file(path=contents.path, message=\"new_model_trained\", content=file_content, sha=contents.sha)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=deploy_to_run, package_path=\"deploy_to_run.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a28ad61-e664-4d4c-86fb-60273a2e1c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'air-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea9e28-c42d-451b-aec6-22167bc6aa94",
   "metadata": {},
   "source": [
    "We are setting the global variables to pass in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cc693-a2a0-429a-94f7-46d4858cf7c3",
   "metadata": {},
   "source": [
    "## Step 3: Defining the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ff1cd10-c9f0-4e8a-8fd9-d833dbe28cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://applied-ai-practice00-bucket/pipeline_root_air/'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Global variables\n",
    "PROJECT_ID=\"applied-ai-practice00\"\n",
    "REGION = \"us-central1\"\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://\"+PROJECT_ID+\"-bucket\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root_air/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f8529-150d-4681-b8c9-1776ac5e883f",
   "metadata": {},
   "source": [
    "Below is the code to define ML pipeline. The pipeline first fetches data from bigquery source, it then trains 3 models sequentially and deploys them to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02445af6-2781-4a67-812c-4a3397fd3ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining pipeline\n",
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"sequential-pipeline\",\n",
    "    \n",
    ")\n",
    "def pipeline(\n",
    "    #url: str = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "    #thresholds_dict_str: str = '{\"roc\":0.8}',\n",
    "    serving_container_image_uri: str = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/mlops-pipelines-container-repository/mlops_base_container:latest\"\n",
    "    ):\n",
    "    \n",
    "    # fetch data\n",
    "    data_op = get_air_data()\n",
    "    \n",
    "    # train and evaluate mulitple models\n",
    "    train_model_op_lstm = sequential_model(model_type=\"LSTM\",dataset= data_op.output)\n",
    "    train_model_op_GRU = sequential_model(model_type=\"GRU\", dataset=data_op.output).after(train_model_op_lstm)\n",
    "    train_model_op_RNN = sequential_model(model_type=\"RNN\", dataset=data_op.output).after(train_model_op_GRU)\n",
    "    \n",
    "    # deploy models\n",
    "    \n",
    "    deploy_task1 = deploy_to_run(\n",
    "    tar_path=train_model_op_lstm.outputs[\"tar_path\"]\n",
    "    )\n",
    "    \n",
    "    deploy_task2 = deploy_to_run(\n",
    "    tar_path=train_model_op_GRU.outputs[\"tar_path\"],\n",
    "    )\n",
    "    \n",
    "    deploy_task3 = deploy_to_run(\n",
    "    tar_path=train_model_op_RNN.outputs[\"tar_path\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7ddf7-a19b-4073-9b61-5ed3dd6ed2f7",
   "metadata": {},
   "source": [
    "Pipeline compiler will compile the pipeline and store the pipeline configuration inside a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a636f01-7f2b-4e5a-a54c-625adb3dadd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"custom_train_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e60cc2fc-6741-41b7-8994-7a8d686bbe7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81122313-6fa7-4a07-a30e-e970e707fffd",
   "metadata": {},
   "source": [
    "## Step 4: Running the Pipeline Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee7356-bf67-4d10-8c86-aa134daab98d",
   "metadata": {},
   "source": [
    "Here we are deploying pipeline job which will be submitted for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "513a5d51-2bdb-4c12-93c7-a786a8be67eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"custom-train-pipeline\",\n",
    "    template_path=\"custom_train_pipeline.json\",\n",
    "    job_id=\"custom-train-pipeline-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a3308-1153-45ce-9d4c-06200b58a4c8",
   "metadata": {},
   "source": [
    "Below code will submit job to create the pipeline, you can use the link at the bottom to view pipeline status. Below link will help us monitor the execution of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc4a2ddb-7a8d-4a97-b8a1-ecfb4aa3b97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/653524927160/locations/us-central1/pipelineJobs/custom-train-pipeline-20240520055057\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/653524927160/locations/us-central1/pipelineJobs/custom-train-pipeline-20240520055057')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-train-pipeline-20240520055057?project=653524927160\n"
     ]
    }
   ],
   "source": [
    "# Submit pipeline job\n",
    "pipeline_job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
